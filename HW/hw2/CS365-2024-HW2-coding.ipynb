{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can import other modules if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Visualize a mixture of Gaussians [15 pts]\n",
    "\n",
    "Consider a mixture of two 2D Gaussian distrbutions as follows $$\n",
    "0.4\\cdot \\mathcal{N}\\left(\\left[\\begin{array}{c} \n",
    "10\\\\\n",
    "2\n",
    "\\end{array}\\right], \\left[\\begin{array}{cc} \n",
    "1, 0\\\\\n",
    "0, 1\n",
    "\\end{array}\\right]\\right) + 0.6\\cdot \\mathcal{N}\\left(\\left[\\begin{array}{c} \n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\\right], \\left[\\begin{array}{cc} \n",
    "8.4, 2.0\\\\\n",
    "2.0, 1.7\n",
    "\\end{array}\\right]\\right)\n",
    "$$ \n",
    "\n",
    "a. Compute the marginal distributions for each dimension.\n",
    "\n",
    "b. Compute the mean for each marginal distribution.\n",
    "\n",
    "c. Compute the mean for the two-dimensional distribution.\n",
    "\n",
    "d. Generate both joint and marginal density of this distribution by taking at least 10 thousand samples. Please refer to [Lab4 notebook](https://piazza.com/class_profile/get_resource/lrgnqll42av781/lsigbj4an34as) for the visualization approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type the answers to first three questions here:\n",
    "\n",
    "(a)\n",
    "\n",
    "(b)\n",
    "\n",
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for question d here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simulation: Drop needles [15 pts]\n",
    "\n",
    "Suppose we have a floor made of parallel strips of wood, each the same width $t$, and we drop a needle with length $l=t$ onto the floor. What is the probability that the needle will lie across a line between two strips?\n",
    "\n",
    "Below is an example of two needles dropped. Needle a falls across a line, while needle b does not.\n",
    "\n",
    "![Example](needle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this coding homework, we will simulate such experiments and connect them with the estimation of $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "The first thing to write is a function *drop_needle*. It simulates dropping a needle onto the floor we described and returns whether the needle lies across a line between two strips. \n",
    "\n",
    "Now the question is how to describe the position of a needle using random variables. The figure below visualizes a needle sampled, with $t=l=1$ (see figure above). Remember that the needle should have an equal probability of landing in any position. In fact, we can uniformly sample the position of the needle's mass center and then uniformly sample the angle formed by the needle and the x-axis. Specifically, we only focus on the mass center's position with respect to (w.r.t.) the x-axis since we can assume the strip is long enough.\n",
    "\n",
    "Besides, we do not need to sample the x-value of the center from $-\\inf$ to $\\inf$. Instead, we can uniformly sample it from $0$ to $2t$. Why is this the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![needleExmple2](needleExmple2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def drop_needle(strip_length, needle_length):\n",
    "    \"\"\"\n",
    "    Simulate dropping a needle on to the floor made of parallel strips of woods.\n",
    "    Return whether the needle lie across a line between two strips.\n",
    "\n",
    "    :return: An Integer that equals to 1 if the needle lie across a line, and 0 otherwise.\n",
    "    \"\"\" \n",
    "    \n",
    "    # write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function run_simulation that calls drop_needle repetitively for n times. The function should return the probability that a dropped needle lies across a line based on the n trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(n, strip_length, needle_length):\n",
    "    \"\"\"\n",
    "    Repeat drop_needle experiment for n times. Return the probability that the needle will lie across a line. \n",
    "\n",
    "    :return: float, the probability that the needle will lie across a line according to the n experiments.\n",
    "    \"\"\" \n",
    "    # Write your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the simulation\n",
    "\n",
    "Run the *run_simulation* function 500 times with parameters n=1000, strip_length=1, and needle_length=1. Each time the function is going to return a probability of the needle lying across the line. Plot a histogram of those 500 probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Email Spam Naive Bayes [35 pts]\n",
    "\n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "For a classification task, we aim to predict a binary label $Y\\in \\{0,1\\}$ of an example given its $d$ dimensional features $X_1,X_2,...,X_d$. Bayes Theorem states the (posterior) porbability of have a label $Y$ given the feature observations as follows:\n",
    "\n",
    "$$P(Y|X_1,X_2,...,X_d) = \\frac{P(Y)*P(X_1,X_2,...,X_d|Y)}{P(X_1,X_2,...,X_d)}$$\n",
    "\n",
    "Note $P(Y)$ is called priori, i.e., probability of seeing label $Y$ without observing the features. To determine the binary label $Y$, we need to compare $P(Y=0|X_1,X_2,...,X_d)$ with $P(Y=1|X_1,X_2,...,X_d)$. In Bayes Theorem, the fractions representing both conditional probabilities have the same denominator $P(X_1,X_2,...,X_d)$. Thus we can ignore the denominator and only focusing on \n",
    "$$P(Y|X_1,X_2,...,X_d) \\propto P(Y)*P(X_1,X_2,...,X_d|Y)$$\n",
    "\n",
    "Next, we make a strong assumption: no pair of features in the dataset are dependent (this is the reason why we call it *Naive* Bayes). Under such assumption, $P(X_1,X_2,...,X_d|Y)=P(X_1|Y)*P(X_2|Y)*...*P(X_d|Y)$. Therefore, we conclude\n",
    "\n",
    "$$P(Y|X_1,X_2,...,X_n) \\propto P(Y)*P(X_1|Y)*P(X_2|Y)*...*P(X_d|Y)$$\n",
    "\n",
    "The classifier will decide what class each input belongs to based on highest probability from the equation above.\n",
    "\n",
    "How do we know the priori $P(Y)$ and the conditional probabilities $P(X_i|Y)$? In supervised classification tasks we have labeled data available (we call it training set). We can thus estimate these probabilities based on the training data.\n",
    "\n",
    "## Overview/Task\n",
    "\n",
    "The goal of this programming assignment is to build a naive bayes classifier from scratch that can determine whether email text should be labled spam or not spam based on its contents\n",
    "\n",
    "## Reminders\n",
    "\n",
    "Please remember that the classifier must be written from scratch; do NOT use any libraries that implement the classifier for you, such as but not limited to sklearn.\n",
    "\n",
    "You CAN, however, use SKlearn to split up the dataset between testing and training.\n",
    "\n",
    "Feel free to look up any tasks you are not familiar with, e.g. the function call to read a csv\n",
    "\n",
    "## Task list/Recommended Order\n",
    "\n",
    "In order to provide some guidance, I am giving the following order/checklist to solve this task:\n",
    "<ol>\n",
    "  <li>Compute the \"prior\": P(Y) for Y = 0 (not spam) and Y = 1 (spam)</li>\n",
    "  <li>Compute the \"likelihood\": $P(X_n|Y)$. In this task, we recommend you to start with setting up a dictionary of words appeared in the training set $W=\\{w_1,w_2\\dots \\}$, and constructing feature set $\\{X_1,X_2\\dots \\}$ where $X_i$ is a binary indicator of whether the word $w_i$ exists in the text or not. Feel free to add preprocessing and modify the features if that can increase your accuracy!</li>\n",
    "  <li>Write code that uses the two items above to make a decision on whether or not an email is spam or ham (aka not spam)</li>\n",
    "  <li>Write code to evaluate your model. Test model on training data to debug </li>\n",
    "  <li>Test model on testing data to debug </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prior(df):\n",
    "    ham_prior = 0\n",
    "    spam_prior =  0\n",
    "    '''YOUR CODE HERE'''\n",
    "\n",
    "\n",
    "    '''END'''\n",
    "    return ham_prior, spam_prior\n",
    "\n",
    "def likelihood(df):\n",
    "    ham_like_dict = {}\n",
    "    spam_like_dict = {}\n",
    "    '''YOUR CODE HERE'''\n",
    "\n",
    "    '''END'''\n",
    "\n",
    "    return ham_like_dict, spam_like_dict\n",
    "\n",
    "def predict(ham_prior, spam_prior, ham_like_dict, spam_like_dict, text):\n",
    "    '''\n",
    "    prediction function that uses prior and likelihood structure to compute proportional posterior for a single line of text\n",
    "    '''\n",
    "    #ham_spam_decision = 1 if classified as spam, 0 if classified as normal/ham\n",
    "    ham_spam_decision = None\n",
    "\n",
    "    '''YOUR CODE HERE'''\n",
    "    #ham_posterior = posterior probability that the email is normal/ham\n",
    "    ham_posterior = None\n",
    "\n",
    "    #spam_posterior = posterior probability that the email is spam\n",
    "    spam_posterior = None\n",
    "\n",
    "    '''END'''\n",
    "    return ham_spam_decision\n",
    "\n",
    "\n",
    "def metrics(ham_prior, spam_prior, ham_dict, spam_dict, df):\n",
    "    '''\n",
    "    Calls \"predict\" function and report accuracy, precision, and recall of your prediction\n",
    "    '''\n",
    "    \n",
    "    '''YOUR CODE HERE'''\n",
    "\n",
    "\n",
    "    '''END'''\n",
    "    return acc, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answers with your functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in the training data\n",
    "train_df = pd.read_csv(\"./TRAIN_balanced_ham_spam.csv\")\n",
    "test_df = pd.read_csv(\"./TEST_balanced_ham_spam.csv\")\n",
    "df = train_df\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the prior\n",
    "\n",
    "ham_prior, spam_prior = prior(df)\n",
    "\n",
    "print(ham_prior, spam_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute likelihood\n",
    "\n",
    "ham_like_dict, spam_like_dict = likelihood(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your predict function with some example TEXT\n",
    "\n",
    "some_text_example = \"write your test case here\"\n",
    "print(predict(ham_prior, spam_prior, ham_like_dict, spam_like_dict, some_text_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test_df and compute metrics \n",
    "    \n",
    "df = test_df\n",
    "acc, precision, recall = metrics(ham_prior, spam_prior, ham_dict, spam_dict, df)\n",
    "print(acc, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gradient Decent [35 pts]\n",
    "\n",
    "## Exact Gradient Computation\n",
    "\n",
    "Given a function f, sometimes we can compute its exact gradient at any x if f's derivative is easy to compute. For example, let $f(x)=2x^2-3x+\\ln x$, where $x>0$. Please compute the derivative of f and report its gradient at $x=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Gradient Computation\n",
    "\n",
    "Instead of computing the derivative of a function, we can also estimate the gradient numerically with various methods. These methods are essential, especially when a callable function is not exposed due to privacy reasons, or it is hard to differentiate analytically. \n",
    "\n",
    "To numerically compute the gradient, the simple way is to follow Newton's difference quotient: $f'(x)=\\lim_{h\\to 0}{f(x+h)-f(x)\\over h}$. Another two-point formula is to compute the slope through the points $(x-h,f(x-h))$ and $(x+h,f(x+h))$. Let us reuse the example function $f(x)=2x^2-3x+\\ln x$ and test the precision of these two approaches. Define the function in the next cell, and try to compute its gradient via both methods at $x=2$. Range h value in [0.1,0.01,0.001,0.0001] and report all gradients calculated. Which method is more accurate, and why does it work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def f(x):\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient using the first method (Newton's difference quotient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient using the second method \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark \n",
    "You may find the gradient more accurate when using a smaller h value. However, this is not always the case. Due to the finite precision of the floating-point, rounding errors always exist and can dominate the computation when the h value is too small. Run the following two cells to observe such scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-15\n",
    "print((f(2+eps)-f(2-eps))/2/eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-20\n",
    "print((f(2+eps)-f(2-eps))/2/eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression is a classification tool that models the probability of an event taking place by having the log odds for the event be a linear combination of one or more independent variables. Specifically, let $\\vec{x}=<x_1,\\dots ,x_m>$ be an m dimensional vector of independent variables (features), and $y$ be the corresponding binary dependent variable (label). The probability of having $y=1$ is modeled as $$P_y={1\\over 1+e^{-(b_0+b_1\\cdot x_1+\\dots +b_m\\cdot x_m)}}={1\\over 1+e^{-(b_0+\\vec{b}_{1:m}\\cdot\\vec{x})}}$$\n",
    "\n",
    "Given a set of data points $<\\vec{x}_k,y_k>$ with $k\\in [1,n]$, how can we fit the model with these data, i.e., how to choose the best $\\vec{b}=b_0,b_1\\cdots,b_m$?\n",
    "\n",
    "One way is to write out the likelihood $$\\prod_{k:y_k=1}P_{y_k}\\prod_{k:y_k=0}(1-P_{y_k})$$ and find $b_0,b_1\\cdots,b_m$ that maximize its logarithm, $$l=\\sum_{k:y_k=1}\\ln(P_{y_k})+\\sum_{k:y_k=0}\\ln(1-P_{y_k})$$\n",
    "\n",
    "In contrast to computing the closed form gradient of a Least-squares loss in a linear model (chapter 5 of MML book), doing the same for logistic regression is not possible. Gradient descent can be used to optimize such function $l$, and we will implement it step-by-step. First, write a function log_likelihood in the next cell that computes the log-likelihood given data points and $\\vec{b}$. [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(X,y,b):\n",
    "    \"\"\"\n",
    "    X: n*m numpy data array.\n",
    "    y: one dimension numpy data array of length n\n",
    "    b: one dimension numpy data array of length m+1\n",
    "    \n",
    "    Return the log likelihood.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your log_likelihood function with a small example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[0.1],[0.5],[1.]])\n",
    "y=np.array([0,0,1])\n",
    "b=np.array([0.,1.])\n",
    "# Your answer should be around -2.03\n",
    "log_likelihood(X,y,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function to maximize, the next step is to compute the gradient of the log-likelihood with respect to parameter $\\vec{b}$. Use the method with Newton's difference quotient, and set $h=0.0001$. Implement the function compute_gradient in the next cell. [7 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X,y,b):\n",
    "# The inputs are the same as the ones of log_likelihood\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function here, preserve the output\n",
    "compute_gradient(X,y,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know how to compute the gradients, we can optimize the objective, which is log-likelihood in our case, using gradient descent. It iteratively changes the parameters in a small \"step\" towards the gradient direction, i.e., the direction where the objective increases at the fastest pace. Formally, denote the calculated gradients as $\\Delta (\\vec{b})$, we can update our parameters via $\\vec{b}=\\vec{b}+\\gamma \\cdot \\Delta (\\vec{b})$, where $\\gamma$ is the size of the \"step\". Repeat this process until the objective stop improving or a pre-set max number of iterations is reached. **Note in practice, the value of gradient changes over iterations and can be very large/small, so you should normalize the gradient vector every iteration, i.e., scale it to $\\Delta (\\vec{b})\\over ||\\Delta (\\vec{b})||_2$, before using it to compute the new $\\vec{b}$. Therefore, the update rule for parameters becomes $\\vec{b}=\\vec{b}+\\gamma \\cdot {\\Delta (\\vec{b})\\over ||\\Delta (\\vec{b})||_2}$**.\n",
    "\n",
    "Implement the gradient_descent function below. [7 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, initial_b, step_size, max_iteration):\n",
    "    \"\"\"\n",
    "    X: n*m numpy data array.\n",
    "    y: one dimension numpy data array of length n\n",
    "    initial_b: one dimension numpy data array of length m+1\n",
    "    step_size: scalar, the size of one step update\n",
    "    max_iteration: scalar, the max number of iterations\n",
    "    Return the updated coefficient vector b.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function with the previous example again. Print for each sample from X, based on your model, the probability of having label=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_b = gradient_descent(X, y, b, 0.1, 1000)\n",
    "\n",
    "# compute and print the probability for each row in X below using optimized_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the implemented logistic regression model to a real dataset. The dataset is a trimmed breast-cancer-Wisconsin dataset from UCI machine learning Repository. Only 100 data points are offered in the training set to make sure the computation can be finished swiftly, no matter how you implement the optimizer. The training dataset is loaded in the next cell, and the vector $\\vec{b}$ is also randomly initialized. \n",
    "\n",
    "Fit three models with the training set using different step size ranging in [0.01,0.05,0.1] and set the max number of iterations as 10000. How do the final log-likelihood value and the number of iterations change with different step sizes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"breast-cancer-wisconsin.data\",\"r\")\n",
    "X_train = []\n",
    "y_train = []\n",
    "for line in f:\n",
    "    tmp = []\n",
    "    for part in line.strip().split(\",\")[1:-1]:\n",
    "        tmp.append(float(part))\n",
    "    y_train.append((0 if line.strip().split(\",\")[-1]==\"2\" else 1))\n",
    "    X_train.append(tmp)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "random_b = np.random.uniform(0,1,size=(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit three models with different step size, report the final log-likelihood, \n",
    "# number of iterations and the final coefficent vector b.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, load the test dataset, and predict for each sample in the test set what labels it should have using the model obtained. Compare your results with the ground truth labels, and report the accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"test_data.txt\",\"r\")\n",
    "X_test = []\n",
    "y_test = []\n",
    "for line in f:\n",
    "    tmp = []\n",
    "    for part in line.strip().split(\",\")[1:-1]:\n",
    "        tmp.append(float(part))\n",
    "    y_test.append((0 if line.strip().split(\",\")[-1]==\"2\" else 1))\n",
    "    X_test.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict based on your models and report the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt",
   "language": "python",
   "name": "gt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
